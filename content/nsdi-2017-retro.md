Title: NSDI 2017 Retro
Date: 2017-07-02 20:01
Author: crossjam
Category: Uncategorized
Slug: nsdi-2017-retro
Status: published

NSDI is the abbreviation for [the USENIX Symposium on Networked Systems Design and Implementation][4]. It’s a highly regarded conference for Systems researchers. I’ve been occasionally scanning the [proceedings for 2017][3], reading a paper here or there.

Some of the folks at the [Stanford DAWN project][1] attended the 2017 meeting and [wrote up their perspective][2]. Definitely provides a different angle from the way I was looking at the conference proceedings: 
> A group of us at DAWN went to NSDI last month. The program was quite diverse, spanning a wide variety of sub-areas in the networking and distributed systems space.

> We were excited to see some trends in the research presented that meshed well with the DAWN vision.

In bullet points the trends were:

* More support for machine learning
* Video as a data source for analytics
* Embracing the use of hardware accelerators and FPGAs
* Frameworks that exploit fine grained parallelism
* High performance with high programmer productivity

DAWN is shaping up to be an interesting project, in the Berkeley CS tradition of highly collaborative research teams bounded by a a 5 year lifespan. Go figure, given [some of the principals][5] involved. 

Also, Matei Zaharia and Peter Bailis popped up on [an ArchiTECHt podcast][6], which was pretty informative. Alex Ratner had [a related discussion with Ben Lorica][7] on generating training data with limited resources.

[1]: http://dawn.cs.stanford.edu/
[2]: http://dawn.cs.stanford.edu/2017/04/28/nsdi/
[3]: https://www.usenix.org/conference/nsdi17
[4]: https://www.usenix.org/conferences/byname/178
[5]: http://dawn.cs.stanford.edu/people/
[6]: https://architecht.io/how-sparks-creator-is-trying-democratize-ai-with-the-dawn-project-672bc98b51f6
[7]: http://practicalquant.blogspot.com/2017/06/creating-large-training-data-sets-quickly.html