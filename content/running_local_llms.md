Title: Running Local LLMs
Date: 2024-06-01
Status: published

Link parkin’: [50+ Open-Source Options for Running LLMs Locally][1]

Vince Lam put together a comprehensive resource for running LLMs on
your own hardware:

> There are many open-source tools for hosting open weights LLMs
> locally for inference, from the command line (CLI) tools to full GUI
> desktop applications. Here, I’ll outline some popular options and
> provide my own recommendations. I have split this post into the
> following sections:

> 1. All-in-one desktop solutions for accessibility
> 2. LLM inference via the CLI and backend API servers
> 3. Front-end UIs for connecting to LLM backends

[GitHub Repo][2], [helpful Google Sheet][3]

[1]: https://medium.com/thedeephub/50-open-source-options-for-running-llms-locally-db1ec6f5a54f
[2]: https://github.com/vince-lam/awesome-local-llms
[3]: https://docs.google.com/spreadsheets/d/1Xv38p90V3GiJXjq0a3qc24056Vicn1I5MG6QiFE6nVE/edit#gid=0
